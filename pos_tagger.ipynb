{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "4Lv2yXLVpQ5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FNN POS tagger\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "from io import open\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "from io import open\n",
        "import gensim.downloader as api\n",
        "file_path = '/content/en_atis-ud-train.conllu'\n",
        "word2vec_model= api.load(\"word2vec-google-news-300\")\n",
        "p = 2\n",
        "s = 2\n",
        "def parse_conllu_fnn(file_path):\n",
        "    word_counts = defaultdict(int)\n",
        "    unique_pos_tags = set()\n",
        "    with open(file_path, 'r', encoding='utf-8') as data_file:\n",
        "        for line in data_file:\n",
        "            if not line.startswith('#') and line.strip():\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) > 3:\n",
        "                    word = parts[1].lower()\n",
        "                    pos_tag = parts[3]\n",
        "                    word_counts[word] += 1\n",
        "                    unique_pos_tags.add(pos_tag)\n",
        "    return word_counts, unique_pos_tags\n",
        "def preprocess_conllu_file_fnn(file_path, p, s, low_freq_words):\n",
        "    input_sequences = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as data_file:\n",
        "        sentence_tokens = []\n",
        "        for line in data_file:\n",
        "            if line.strip() and not line.startswith('#'):\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) > 3:\n",
        "                    word = parts[1].lower()\n",
        "                    pos_tag = parts[3]\n",
        "                    word = word if word not in low_freq_words else '<unknown>'\n",
        "                    sentence_tokens.append((word, pos_tag))\n",
        "            elif sentence_tokens:\n",
        "                words = ['<start>'] + ['<PAD>'] * (p-1) + [token[0] for token in sentence_tokens] + ['<PAD>'] * (s-1) + ['<end>']\n",
        "                pos_tags = ['<PAD>'] * (p-1) + [token[1] for token in sentence_tokens] + ['<PAD>'] * (s-1)\n",
        "                for i in range(len(sentence_tokens)):\n",
        "                    segment_start = i\n",
        "                    segment_end = i + p + s\n",
        "                    segment = words[segment_start:segment_end+1]\n",
        "                    label = pos_tags[i + p - 1]\n",
        "                    input_sequences.append(segment)\n",
        "                    labels.append(label)\n",
        "                sentence_tokens = []\n",
        "    return input_sequences, labels\n",
        "def pos_tag_to_one_hot(tag, pos_tag_to_index):\n",
        "    one_hot_vector = np.zeros(len(pos_tag_to_index))\n",
        "    one_hot_vector[pos_tag_to_index[tag]] = 1\n",
        "    return one_hot_vector\n",
        "def get_word_embedding(word, word2vec_model):\n",
        "    if word in word2vec_model.key_to_index:\n",
        "        return word2vec_model[word]\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n",
        "def preprocess_with_embeddings_and_one_hot(input_sequences, labels, model, pos_tag_to_index):\n",
        "    embeddings_sequences = []\n",
        "    one_hot_labels = []\n",
        "    for sequence in input_sequences:\n",
        "        embeddings_sequence = [get_word_embedding(word, model) for word in sequence]\n",
        "        embeddings_sequences.append(embeddings_sequence)\n",
        "    for label in labels:\n",
        "        one_hot_label = pos_tag_to_one_hot(label, pos_tag_to_index)\n",
        "        one_hot_labels.append(one_hot_label)\n",
        "    return np.array(embeddings_sequences), np.array(one_hot_labels)\n",
        "def preprocess_sentence_fnn(sentence, model, p, s, low_frequency_words=set('<unknown>')):\n",
        "    tokens = ['<start>'] + ['<PAD>'] * (p - 1) + sentence.lower().split() + ['<PAD>'] * (s - 1) + ['<end>']\n",
        "    tokens = [token if token not in low_frequency_words else '<unknown>' for token in tokens]\n",
        "    embeddings = [get_word_embedding(token, model) for token in tokens]\n",
        "    input_chunks = []\n",
        "    for i in range(p, len(tokens) - s):\n",
        "        context_window = embeddings[i - p:i + s + 1]\n",
        "        input_chunks.append(context_window)\n",
        "    return np.array(input_chunks)\n",
        "\n",
        "def predict_pos_tags_fnn(sentence, model, word2vec_model, pos_tag_to_index, p, s, low_frequency_words):\n",
        "    input_chunks = preprocess_sentence_fnn(sentence, word2vec_model, p, s, low_frequency_words)\n",
        "    predicted_pos_tags = []\n",
        "    for chunk in input_chunks:\n",
        "        embeddings_flattened = chunk.flatten().reshape(1, -1)\n",
        "        input_tensor = torch.tensor(embeddings_flattened, dtype=torch.float)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "            prediction = torch.argmax(outputs, dim=1)\n",
        "        index_to_pos_tag = {v: k for k, v in pos_tag_to_index.items()}\n",
        "        predicted_pos_tags.append(index_to_pos_tag[prediction.item()])\n",
        "    return predicted_pos_tags\n",
        "\n",
        "class POSFFNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
        "        super(POSFFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim * (p + s + 1), hidden_dim )\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim , hidden_dim)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "def pos_with_fnn(sentence):\n",
        "    word_counts, unique_pos_tags = parse_conllu_fnn(file_path)\n",
        "    low_frequency_words = {word for word, count in word_counts.items() if count < 3}\n",
        "    input_sequences, labels = preprocess_conllu_file_fnn(file_path, p, s, low_frequency_words)\n",
        "    pos_tag_to_index = {tag: idx for idx, tag in enumerate(sorted(unique_pos_tags))}\n",
        "    model = POSFFNN(word2vec_model.vector_size, 512, len(pos_tag_to_index))\n",
        "    model = torch.load('/content/pos_ffnn_model .pth')\n",
        "    model.eval()\n",
        "    predicted_pos_tags = predict_pos_tags_fnn(sentence, model, word2vec_model, pos_tag_to_index, p, s, set('<unknown>'))\n",
        "    tokens = sentence.split()\n",
        "    for token, tag in zip(tokens, predicted_pos_tags):\n",
        "        print(f'{token} -> {tag}')\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "pos_with_fnn(sentence)"
      ],
      "metadata": {
        "id": "DnO_gyqWpY9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19hdkpfWpFOL"
      },
      "outputs": [],
      "source": [
        "#LSTM POS tagger\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from conllu import parse_incr\n",
        "\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "file_path = '/content/en_atis-ud-train.conllu'\n",
        "word_counts = defaultdict(int)\n",
        "unique_pos_tags = set()\n",
        "def parse_conllu(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as data_file:\n",
        "        for line in data_file:\n",
        "            if not line.startswith('#') and line.strip():\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) > 3:\n",
        "                    word = parts[1].lower()\n",
        "                    pos_tag = parts[3]\n",
        "                    word_counts[word] += 1\n",
        "                    unique_pos_tags.add(pos_tag)\n",
        "parse_conllu(file_path)\n",
        "vocabulary = {word for word, count in word_counts.items() if count >= 3}\n",
        "def replace_low_freq_words(sentence):\n",
        "    return ['<UNK>' if word.lower() not in vocabulary else word.lower() for word in sentence]\n",
        "def prepare_sequence(seq, to_ix, is_pos=False):\n",
        "    if is_pos:\n",
        "        idxs = [to_ix.get(word, to_ix['<UNK>']) for word in seq]\n",
        "    else:\n",
        "        idxs = [to_ix.get(word, to_ix['<UNK>']) for word in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "def get_word_vector(word, model):\n",
        "    if word in model.key_to_index:\n",
        "        return model[word]\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "def sentences_to_vectors(padded_sentences, word_to_ix, model):\n",
        "    sentence_vectors = torch.zeros(padded_sentences.size(0), padded_sentences.size(1), model.vector_size)\n",
        "    for i, sentence in enumerate(padded_sentences):\n",
        "        for j, word_idx in enumerate(sentence):\n",
        "            word = list(word_to_ix.keys())[list(word_to_ix.values()).index(\n",
        "                word_idx.item())] if word_idx.item() in list(word_to_ix.values()) else \"<UNK>\"\n",
        "            word_vector = get_word_vector(word, model)\n",
        "            sentence_vectors[i, j, :] = torch.tensor(word_vector)\n",
        "    return sentence_vectors\n",
        "data_file = open(\"/content/en_atis-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "sentences = []\n",
        "pos_tags = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "    sentence_tokens = []\n",
        "    sentence_pos_tags = []\n",
        "    for token in tokenlist:\n",
        "        sentence_tokens.append(token['form'])\n",
        "        sentence_pos_tags.append(token['upos'])\n",
        "    sentences.append(replace_low_freq_words(sentence_tokens))\n",
        "    pos_tags.append(sentence_pos_tags)\n",
        "max_sentence_length = max(len(sentence) for sentence in sentences)\n",
        "pos_tag_vocab = {tag: idx for idx, tag in enumerate(unique_pos_tags)}\n",
        "pos_tag_vocab['<PAD>'] = len(pos_tag_vocab)\n",
        "pos_tag_vocab['<UNK>'] = len(pos_tag_vocab)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocabulary, start=1)}\n",
        "word_to_ix['<UNK>'] = 0\n",
        "word_to_ix['<PAD>'] = len(word_to_ix)\n",
        "indexed_sentences = [prepare_sequence(sentence, word_to_ix) for sentence in sentences]\n",
        "indexed_pos_tags = [prepare_sequence(tags, pos_tag_vocab, is_pos=True) for tags in pos_tags]\n",
        "pad_token = word_to_ix['<PAD>']\n",
        "pad_tag = pos_tag_vocab['<PAD>']\n",
        "padded_sentences = pad_sequence(indexed_sentences, batch_first=True, padding_value=pad_token)\n",
        "padded_pos_tags = pad_sequence(indexed_pos_tags, batch_first=True, padding_value=pad_tag)\n",
        "sentence_vectors = sentences_to_vectors(padded_sentences, word_to_ix, word2vec_model)\n",
        "class LSTMPOSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMPOSTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=3, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "        self.elu = nn.ELU()\n",
        "    def forward(self, sentence):\n",
        "        lstm_out, _ = self.lstm(sentence)\n",
        "        tag_space = self.hidden2tag(self.elu(lstm_out))\n",
        "        tag_scores = nn.functional.log_softmax(tag_space, dim=2)\n",
        "        return tag_scores\n",
        "model_file_path = '/content/lstm_pos_tagger_model.pth'\n",
        "checkpoint = torch.load(model_file_path)\n",
        "loaded_model = LSTMPOSTagger(embedding_dim=checkpoint['input_dim'], hidden_dim=checkpoint['hidden_dim'],vocab_size=len(checkpoint['word_to_ix']),tagset_size=checkpoint['output_dim'])\n",
        "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "word_to_ix = checkpoint['word_to_ix']\n",
        "pos_tag_vocab = checkpoint['pos_tag_vocab']\n",
        "loaded_model.eval()\n",
        "def predict_sentence_tags(sentence, model, word_to_ix, pos_tag_vocab, word2vec_model):\n",
        "    preprocessed_sentence = replace_low_freq_words(sentence.split())\n",
        "    indexed_sentence = prepare_sequence(preprocessed_sentence, word_to_ix)\n",
        "    sentence_padded = pad_sequence([indexed_sentence], batch_first=True, padding_value=word_to_ix['<PAD>'])\n",
        "    sentence_vectors = sentences_to_vectors(sentence_padded, word_to_ix, word2vec_model)\n",
        "    with torch.no_grad():\n",
        "        tag_scores = model(sentence_vectors)\n",
        "        predicted_tags_idx = torch.argmax(tag_scores, dim=2).squeeze()\n",
        "    idx_to_tag = {idx: tag for tag, idx in pos_tag_vocab.items()}\n",
        "    predicted_tags = [idx_to_tag.get(idx.item(), '<UNK>') for idx in predicted_tags_idx]\n",
        "    return list(zip(sentence.split(), predicted_tags))\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "predicted_tags = predict_sentence_tags(sentence, loaded_model, word_to_ix, pos_tag_vocab, word2vec_model)\n",
        "for word, tag in predicted_tags:\n",
        "    print(f'{word} -> {tag}')"
      ]
    }
  ]
}